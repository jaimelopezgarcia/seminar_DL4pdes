{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc48c41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Deep Learning for PDEs\n",
    "### Jaime López García, Repsol\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554bb337",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Contents\n",
    "\n",
    "\n",
    "- [Introduction](#the_destination)\n",
    "  * [Why deep learning and pdes](#sub-heading)\n",
    "  * [Applications](#sub-heading)\n",
    "  * [Solving PDEs with DL, overview](#sub-sub-heading)\n",
    "- [DL, a modern recipe](#heading-1)\n",
    "  * [DL basics](#sub-heading-1)\n",
    "  * [Backpropagation (it's just good'ol adjoint method)](#sub-heading-1)\n",
    "  * [Going deep, main problems, main solutions](#sub-heading-1)\n",
    "  * [Practicioner recipe](#sub-heading-1)\n",
    "- [Learning from data](#heading-2)\n",
    "  * [Problem Statement](#sub-heading-2)\n",
    "  * [Case study Allen Cahn](#sub-sub-heading-2)\n",
    "  * [Multiple scales](#whatever)\n",
    "  * [Results](#hehe)\n",
    "      + [Validation dataset time predictions](#hhoo)\n",
    "      + [Time errors](#hoho)\n",
    "      + [Resolution dependency](#hoho)\n",
    "  * [Code disection](#huhu)\n",
    "- [Unsupervised Learning of the PDE, PINNS](#huhu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0727b38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "## Why Deep Learning and PDEs\n",
    "\n",
    "\n",
    "Deep learning has proven to be an invaluable  tool to learn arbitrarly complex functions by mapping the data into  a manifold of much  lower dimension than the original space, encoding semantic variations continuously in the coordinates of this latent space.\n",
    "\n",
    "If this works with  music and images, it is expected to perform even better with mathematical entities that  live in a continuous and smooth manifold, as it is the case with the solution of PDEs.\n",
    "\n",
    "  Latent space           |  Normal modes\n",
    ":-------------------------:|:-------------------------:\n",
    "<img width=\"600\" height=\"600\" style = \"float : right\" src=\"media/Intro_manifold.png\">  |  <img width=\"600\" height=\"600\" style = \"float : right\" src=\"media/intro_modes.gif\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2fe9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "## Example applications\n",
    "\n",
    "The most inmediate contribution of DL to PDEs problems relies on the reduction of time complexity.This entails inmediate applications:\n",
    "   * Inverse problems and control processes. <a href=\"https://arxiv.org/abs/2001.07457\"><cite style=\"font-size:15px\">Learning to Control PDEs with Differentiable Physics</cite>.</a>\n",
    "   * Computer graphics  <a href=\"https://arxiv.org/abs/1806.02071\"><cite style=\"font-size:15px\">Deep Fluids: A Generative Network for Parameterized Fluid Simulations</cite>.</a>\n",
    "   * Fast prototyping, assisted engineering. <a href=\"https://arxiv.org/pdf/1810.08217.pdf\"><cite style=\"font-size:15px\">Deep learning methods for Reynolds-averaged Navier–Stokes simulations of airfoil flows</cite>.</a>\n",
    "\n",
    "Other contributions beside this should be mentioned\n",
    "   * Explainability   <a href=\"https://www.nature.com/articles/s41467-018-07210-0\"><cite style=\"font-size:15px\">Deep learning for universal linear embeddings of nonlinear dynamics</cite>.</a>\n",
    "   * Data assimilation  <a href=\"https://www.sciencedirect.com/science/article/pii/S0021999118307125\"><cite style=\"font-size:15px\">M. Raissi, P. Perdikaris, G.E. Karniadakis, PINNS</cite>.</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a2475",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "## How do we solve PDEs with deep learning\n",
    "### Two general approaches\n",
    "\n",
    "   - **Supervised learning approach**: Sample data from the population of solutions, and make the neural network learn the mapping $NN: parameter \\rightarrow solution$.  <a href=\"https://arxiv.org/abs/2010.08895\"><cite style=\"font-size:15px\">Fourier Neural Operator for Parametric Partial Differential Equations</cite>.</a>\n",
    "\n",
    "   - **Weighted residuals approach**: Reparametrice $ N_{p}(y,\\frac{\\partial y}{\\partial x},...) = 0 $ with a neural network $\\hat{y}(x,\\theta)$ and minimize the residues of the associated functional along with the BCs. <a href=\"https://www.sciencedirect.com/science/article/pii/S0021999118307125\"><cite style=\"font-size:15px\">M. Raissi, P. Perdikaris, G.E. Karniadakis, PINNS</cite>.</a>\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af004400",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning, a modern recipe\n",
    "## Neural network basics\n",
    "### Dont fix the basis, learn it\n",
    "\n",
    "Example, least squares regression problem $ \\underset{w}{argmin}L(w) = \\underset{w}{argmin}\\sum_{Data}|y-\\hat{y}(x,w)|^{2}$\n",
    " * Feature engenering, \"shallow learning\": $\\hat{y} = \\sum_{i}w_{i}\\phi_{i}(x) $\n",
    " * Neural networks : $\\hat{y} = \\sum_{i}w_{i}\\phi_{i}(x, w) $\n",
    " \n",
    "Instead of just addition as a building mechanism, NN use composition of blocks of non linear functions and linear applications grouped in **layers**.\n",
    "\\begin{equation}\n",
    "\\phi(w,x) = \\sigma(W_{L} \\sigma(W_{L-1}....\\sigma(W_{1}X) )\n",
    "\\end{equation}\n",
    " \n",
    "To optimize the model, we need to calculate the gradients $\\partial_{w}L$ respect to the weights of every layer. The algorithm that does it is a special implementation  of the **adjoint method**, called **backpropagation**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38a9b61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning, a modern recipe\n",
    "## Neural network basics\n",
    "### Calculating gradients\n",
    "#### Reformulation as a constrained optimization problem\n",
    "\n",
    "\\begin{array}{lc}\n",
    "\\mbox{J}: \\underset{p}{\\mbox{minimize}} & \\sum(x_{Data}-\\hat{x}(x_{input},p))^{2}  \\\\\n",
    "\\end{array}\n",
    "\\begin{array}{lm}\n",
    "\\mbox{G}: &   \\begin{cases} \n",
    "      \\hat{x} = \\sigma(Wx_{k})  \\\\\n",
    "      x_{k} = \\sigma(W_{k}x_{k-1}) \\\\\n",
    "      \\vdots \\\\\n",
    "      x_{2} = \\sigma(W_{2}x_{1})\\\\\n",
    "      x_{1} = \\sigma(W_{1}x_{input}) \\\\\n",
    "   \\end{cases} \\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801007cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning, a modern recipe\n",
    "## Neural network basics\n",
    "### Calculating gradients\n",
    "#### Adjoint method\n",
    "\\begin{array}{lc}\n",
    "\\underset{p}{\\mbox{minimize}} & J(x_{T},p)  \\\\\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{lcm}\n",
    "\\mbox{forward system} & g(x,p) = 0 = &   \\begin{cases} \n",
    "      g(x_{T},x_{T-1},p) = 0  \\\\\n",
    "      g(x_{T-1},x_{T-2},p) = 0 \\\\\n",
    "      \\vdots \\\\\n",
    "      g(x_{2},x_{1},p) = 0\\\\\n",
    "      g(x_{1},x_{0},p) = 0 \\\\\n",
    "      g(x_{0},p) = 0\n",
    "   \\end{cases} \\\\\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "J_{aug}(x_{T},p) = J(x_{T},p) + \\lambda_{T}g(x_{T},x_{T-1},p) + \\lambda_{T-1}g(x_{T-1},x_{T-2},p) \\ldots \\lambda_{1}g(x_{1},x_{0},p)+ \\lambda_{0}g(x_{0},p)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{array}{lm}\n",
    "\\mbox{backward/adjoint system} & \\begin{cases}\n",
    "        \\partial_{x}J+\\lambda_{T}\\partial_{x_{T}}g_{T} = 0 \\\\\n",
    "        \\lambda_{T}\\partial_{x_{T-1}}g_{T}+\\lambda_{T-1}\\partial_{x_{T-1}}g_{T-1} =0 \\\\\n",
    "        \\vdots \\\\\n",
    "        \\lambda_{1}\\partial_{x_{0}}g_{1}+\\lambda_{0}\\partial_{x_{0}}g_{0} = 0 \\\\\n",
    "        \\end{cases} \\\\\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{lm}\n",
    "\\mbox{Gradient} & \\begin{equation}\n",
    "        d_{p} J_{aug} = \\partial_{p} J +\n",
    "        \\lambda_{T}\\partial_{p}g_{T}+\\lambda_{T-1}\\partial_{p}g_{T-1}+\\ldots +\n",
    "        \\lambda_{1}\\partial_{p}g_{1}+\n",
    "        \\lambda_{0}\\partial_{p}g_{0}\n",
    "        \\end{equation}\n",
    "\\end{array}\n",
    "\n",
    "if we use $g_{L} =x_{L}-\\sigma(W_{L}x_{L-1}) = 0$ and clear $\\lambda$ we get the $Backpropagation$ algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a33e17",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning, a modern recipe\n",
    "## Neural network basics\n",
    "### Calculating gradients\n",
    "#### Adjoint method in time dependent problems\n",
    "\n",
    "\n",
    "\\begin{array}{lc}\n",
    "\\underset{p}{\\mbox{minimize}} & J(x(t),p) = \\int_{0}^{T} f(x(t),p) \\,dt  \\\\\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{lc}\n",
    "\\mbox{forward} & \\begin{cases} \n",
    "      g(\\dot{x},x,p) = 0  \\\\\n",
    "      g_{0}(x_{0},p) = 0\n",
    "   \\end{cases} \\\\\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{lc}\n",
    "\\mbox{backward/adjoint system} & \\begin{cases} \n",
    "       \\dot{\\lambda(\\tau)}\\partial_{\\dot{x}}g - \\lambda(\\tau)\\dot{\\partial_{\\dot{x}}}g + \\lambda(\\tau)\\partial_{x}g = -\\partial_{x}f  \\\\\n",
    "     \\lambda(\\tau = 0) = 0 \\\\\n",
    "      \\mu=\\lambda\\partial_{\\dot{x}}g\\partial_{x}g_{0}^{-1}\n",
    "   \\end{cases} \\\\\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{lc}\n",
    "\\mbox{Gradient} & \\begin{equation}\n",
    " d_{p}J_{aug} = \\int_{0}^{T}  \\partial_{p}f+\\lambda\\partial_{p}gdt+\\mu\\partial_{p}g_{0}\\Bigr|_{t = 0}\n",
    " \\end{equation} \\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da19395",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning, a modern recipe\n",
    "## Main problems, main solutions\n",
    "The main problem we encounter as we try to fit networks with increasing numbers of layers, is the explosion/vanishing variance of the activations and gradients of different layers.\n",
    " * **PROBLEM**: Decaying and exploding variance. Main techniques to stabilize the network and control variance:\n",
    "  * Proper initialization of weights.\n",
    "  * Normalization ( mostly batchnorm).\n",
    "  \n",
    "  <br/>\n",
    " * **PROBLEM**: Saturation.\n",
    "  * Residual connections    \n",
    "  <br/>\n",
    " * **PROBLEM**: Optimization in a bumpy loss landscape.\n",
    "  * Stochastic gradient descent with momentum , adaptative and annealed learning rate.\n",
    "  \n",
    " <br/>\n",
    "\n",
    "  \n",
    " <br/>\n",
    " \n",
    "* **PROBLEM**: Overfitting.\n",
    "   * Depending on the application it is not that much of a problem. \n",
    "   * Regularization, L2, dropout...\n",
    "  \n",
    "      \n",
    "<a href=\"https://towardsdatascience.com/its-necessary-to-combine-batch-norm-and-skip-connections-e92210ca04da\"><cite style=\"font-size:15px\">On batch norm and skip connections</cite>.</a>\n",
    "  \n",
    "\n",
    "\n",
    "<a href=\"https://openai.com/blog/deep-double-descent/\"><cite style=\"font-size:15px\">On overfitting</cite>.</a>\n",
    "\n",
    "\n",
    "<a href=\"http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\"><cite style=\"font-size:15px\"> On initialization</cite>.</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9400d67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning, a modern recipe\n",
    "## Recipe for building block/layer  and optimizer\n",
    "\n",
    "\n",
    "* Use resnet (with different lengths) as a template and use as much capacity as you can afford.\n",
    "* Main building block consist in layers with residual skip connections and batch normalization.\n",
    "* If using Relu weights must be initializated with the proper scale ( He initialization). \n",
    "* Scale input and labels and use Adam optimization with default parameters (pytorch), and small (1e-5) weight decay and reduce on plateau scheduler.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"500\" height=\"500\" style = \"float : right\" src=\"media/basic_block.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c9b47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Problem statement\n",
    "\n",
    "We consider the PDE $P$ with BC as a mapping $\\psi$ between function spaces where $X$ is the parameter space and $Y$ the solution space.\n",
    "$$P_{X}(y) = 0$$\n",
    "\n",
    "$F$ and $G$ are the operators that project the data to a discrete space. The symbol $\\varphi$ represent the mapping in the discrete space.\n",
    "\n",
    "<img width=\"1300\" height=\"800\" src=\"media/scheme_operator.jpg\">\n",
    "\n",
    "\n",
    "If we work directly in the discretized space, we'll model the mapping with a convolutional neural network by minimizing:\n",
    "$$ \\underset{\\theta}{argmin} \\underset{x \\sim \\mu}{E}(Cost(\\varphi_{solver}(x)-\\hat{\\varphi}(x,\\theta))$$\n",
    "\n",
    "If we work in a function space we'll minimize:\n",
    "$$ \\underset{\\theta}{argmin} \\underset{x \\sim \\mu}{E}(Cost(\\psi_{solver}(x)-\\hat{\\psi}(x,\\theta))$$\n",
    "\n",
    "Both methods work with discrete data , but in the first case , we are learning directly a mapping  in $R^{N_{grid}}$ while in the second case we first project to a function space (fourier transform), we learn the mapping there and transform back to the discretized space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0941871a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Case study : Evolutive system, Allen Cahn, spinodal decomposition\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "    \\partial_{t}u-M(\\Delta u -\\frac{1}{\\epsilon^{2}}(u^{2}-1)u) = 0 \\\\\n",
    "     u,\\nabla u |_{\\partial \\Omega} \\quad periodic \\\\\n",
    "     u(0,x,y) = u_{0}(x,y)\\\\\n",
    "     x,y\\in[0,1]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "  Gibs free energy vs phase            |  Initial condition, small fluctuations that trigger the decomposition\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"media/gibbs_potential.jpg\" alt=\"drawing\" width=\"600\" height=\"600\"  />  |  <img src=\"media/noise.png\" alt=\"drawing\" width=\"600\" height=\"600\"  />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716ad39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Case study : Evolutive system, Allen Cahn, spinodal decomposition\n",
    "\n",
    "  Simulations samples      |  $ M = 1,\\epsilon = 0.01, T = 200 dt$\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"media/sample1.gif\"  width=\"600\" height=\"600\" />  |  <img src=\"media/sample2.gif\"  width=\"600\" height=\"600\" />\n",
    "<img src=\"media/sample3.gif\"  width=\"600\" height=\"600\" />  |  <img src=\"media/sample4.gif\"  width=\"600\" height=\"600\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0979f07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Case study : Evolutive system, Allen Cahn, spinodal decomposition\n",
    "\n",
    "This is an interesting problem to learn beacuse without being chaotic , it exhibits multiple spatial and temporal timescales that must be solved simultaneously to accurattely predict long term behaviour.\n",
    "\n",
    "We have a extremely fast destabilization at the beggining that is followed by a slow evolution guided by the interface advances. Even if the coalescence stage is generally slow, when two drops are close to each other, the blending is fast, so it must be captured with a sufficiently small time step\n",
    "\n",
    "\n",
    "   time evolution   |  $E(abs(phase))\\quad vs \\quad time$\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"media/sample_decomp.gif\" width = \"900\" height = \"900\"  />  |  <img src=\"media/sample_decomp_phase.png\"  width = \"1200\"  height = \"1200\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b6152",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Models architecture\n",
    "\n",
    "   Image-Image CNN   |  Fourier Neural Operator\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"media/unetlike.png\" width = \"1200\" height = \"900\"  />  |  <img src=\"media/fourier_operator.jpg\"  width = \"1200\"  height = \"900\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e18fd8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Training\n",
    "\n",
    "\n",
    "The mapping we'll try to learn is $\\Psi: u_{T-\\Delta t}\\rightarrow u_{T} $ with the goal of applying it recurrently to predict evolution times much longer than $\\Delta t$. As NNs operate in a different space than the original one, they'll not be constrained by the time integration errors of traditional schemes, so larger prediction times can be used. For training we use large $\\Delta t, 6*dt_{solver}$.\n",
    "\n",
    "\n",
    "The objetive is to minimize:\n",
    "$$\\underset{\\theta}{argmin}\\underset{u_{0},T}{E}(|u_{T+\\Delta t}-\\hat{u}_{T+\\Delta t}(\\theta, u_{T})|^{2})$$\n",
    "\n",
    "We compute with Fenics 200 simulations with different random initial conditions of $\\sigma$ 0.02, we split it in a 140:60 training/validation dataset. For validation we skip the first steps so the noise is diffused and the emergence of patterns can be appreciated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf5194",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Results on validation dataset\n",
    "\n",
    "  FeniCS      |  FNO |  CNN\n",
    ":-------------------------:|:-------------------------: |:-------------------------:\n",
    "<img src=\"media/real3.gif\"   width = \"400\" height = \"400\"/>  |  <img src=\"media/pred3fno.gif\" width = \"400\" height = \"400\"  /> |  <img src=\"media/pred3cnn.gif\"  width = \"400\" height = \"400\" />\n",
    "<img src=\"media/real12.gif\" width = \"400\" height = \"400\" />  |  <img src=\"media/pred12fno.gif\" width = \"400\" height = \"400\"  />   |  <img src=\"media/pred12cnn.gif\" width = \"400\" height = \"400\" />\n",
    "<img src=\"media/real13.gif\"  width = \"400\" height = \"400\" />  |  <img src=\"media/pred13fno.gif\"  width = \"400\" height = \"400\" />   |  <img src=\"media/pred13cnn.gif\"  width = \"400\" height = \"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b82b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Models error in time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  CNN,FNO error vs time     |  FNO error time\n",
    ":-------------------------:|:-------------------------: \n",
    "<img src=\"media/two_models_time_error.png\"   />  |  <img src=\"media/time_error.png\"   />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca442c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Long term accuracy, multiscale approach\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/2008.09768\"><cite style=\"font-size:15px\">Hierarchical Deep Learning of Multiscale Differential Equation Time-Steppers</cite>.</a>\n",
    "\n",
    "\n",
    "  Approach   |  different $\\Delta t$ errors comparison\n",
    ":-------------------------:|:-------------------------: \n",
    "<img src=\"media/multiscale_1.jpg\"   />  |  <img src=\"media/multiscale_2.jpg\"   />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c14b63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## FNO phase average comparison\n",
    "\n",
    "\n",
    "  phase quantity vs time     |  2D evolution\n",
    ":-------------------------:|:-------------------------: \n",
    "<img src=\"media/phases_26.png\"  width = \"700\" height = \"700\" />  |  <img src=\"media/sim_comparative_t0_10_26.gif\"  width = \"700\" height = \"700\" />\n",
    "<img src=\"media/phases_38.png\"  width = \"700\" height = \"700\" />  |  <img src=\"media/sim_comparative_t0_10_38.gif\" width = \"700\" height = \"700\"  />  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369bc5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Spinal decomposition from noise\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"media/sim_comparative_t0_0_4.gif\"  width = \"600\" height = \"400\" style = \"float : left\" />  \n",
    "<img src=\"media/sim_comparative_t0_0_14.gif\" width = \"600\" height = \"400\" style = \"float : left\" />  \n",
    "<img src=\"media/sim_comparative_t0_0_16.gif\" width = \"600\" height = \"400\" style = \"float : left\" />  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a55a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Corrupted input\n",
    "\n",
    "\n",
    " original     |  downsampled /2 | sampled 25%   | Gaussian noise $\\sigma = 1$ \n",
    ":-------------------------:|:-------------------------: |:-------------------------: |:-------------------------: \n",
    "<img src=\"media/2_vanilla.gif\"   width = \"400\" height = \"400\"/>  | <img src=\"media/2_downsampled.gif\"  width = \"400\" height = \"400\" />  | <img src=\"media/2_sampled.gif\" width = \"400\" height = \"400\"  />  | <img src=\"media/2_corrupted.gif\" width = \"400\" height = \"400\" width = \"400\" height = \"400\" />  |\n",
    "<img src=\"media/14_vanilla.gif\"  width = \"400\" height = \"400\" />  | <img src=\"media/14_downsampled.gif\"  width = \"400\" height = \"400\" />  | <img src=\"media/14_sampled.gif\"  width = \"400\" height = \"400\" />  | <img src=\"media/14_corrupted.gif\" width = \"400\" height = \"400\"  />  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb3508",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Code dissection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49625842",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-91e22df14f5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSpectralConv2d_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpectralConv2d_fast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class SpectralConv2d_fast(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d_fast, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, 2))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.rfft(x, 2, normalized=True, onesided=True)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.in_channels, x.size(-2), x.size(-1)//2 + 1, 2, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.irfft(out_ft, 2, normalized=True, onesided=True, signal_sizes=(x.size(-2), x.size(-1)))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "122d5c66",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bd39f0bfb2d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSimpleBlock2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_channel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSimpleBlock2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class SimpleBlock2d(nn.Module):\n",
    "    def __init__(self, modes1, modes2, width, input_channel = 3):\n",
    "\n",
    "        super(SimpleBlock2d, self).__init__()\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.fc0 = nn.Linear(input_channel, self.width)\n",
    "\n",
    "        self.conv0 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "\n",
    "        self.w0 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.bn0 = torch.nn.BatchNorm2d(self.width)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(self.width)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        batchsize = x.shape[0]\n",
    "        size_x, size_y = x.shape[1], x.shape[2]\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2) ## to standard torch batch channel X,Y\n",
    "\n",
    "        \n",
    "        \n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x.view(batchsize, self.width, -1)).view(batchsize, self.width, size_x, size_y) #why 1D? just parameter saving?\n",
    "        x = self.bn0(x1 + x2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        \n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x.view(batchsize, self.width, -1)).view(batchsize, self.width, size_x, size_y)\n",
    "        x = self.bn1(x1 + x2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "\n",
    "        x = x.permute(0, 2, 3, 1)  ## from standard torch batch channel X,Y back to batch X,Y channel\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ad663",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Unsupervised learning\n",
    "##  Physical informed neural networks\n",
    "\n",
    "\n",
    "$ Loss(\\theta) = \\int_{\\Omega}||L(x,\\hat{u}(x,\\theta),\\partial_{x}\\hat{u}(x,\\theta)||d\\Omega + \\int_{\\partial \\Omega}||\\hat{u}(x,\\theta)-u_{\\partial \\Omega}||d\\Omega$\n",
    "\n",
    "$ Loss(\\theta) = \\frac{1}{N_{\\Omega}}\\sum_{N_{\\Omega}}||L(x,\\hat{u}(x,\\theta),\\partial_{x}\\hat{u}(x,\\theta), ... )|| + \\frac{1}{N_{\\partial \\Omega}}\\sum_{N_{\\partial \\Omega}}||\\hat{u}(x,\\theta)-u_{\\partial \\Omega}||$\n",
    "\n",
    "<a href=\"https://maziarraissi.github.io/research/09_hidden_fluid_mechanics/\"><cite style=\"font-size:15px\">Hidden Fluid Mechanics</cite>.</a>\n",
    "<a href=\"https://arxiv.org/pdf/2006.09661.pdf\"><cite style=\"font-size:15px\">Implicit Neural Representations with Periodic Activation Functions</cite>.</a>\n",
    "\n",
    "\n",
    "\n",
    "  Batch samples   |  Model\n",
    ":-------------------------:|:-------------------------: \n",
    "<img src=\"media/batch_pinns.gif\"  width = \"300\" height = \"300\" />  |  <img src=\"media/pinns_raissi.jpg\"   width = \"900\" height = \"900\"  />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616751ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Unsupervised learning\n",
    "##  Physical informed neural networks\n",
    "### Poisson equation  $\\Delta u = 0.2(sin(9x)+sin(5y))$\n",
    "\n",
    "$w_{0} = 1.5$   |  $w_{0} = 8$ | $w_{0} = 20$\n",
    ":-------------------------:|:-------------------------: |:-------------------------: |\n",
    "<img src=\"media/lower_freq.gif\"   width = \"1200\" height = \"1600\"/>  | <img src=\"media/close_freq.gif\"  width = \"1200\" height = \"1600\"/>  | <img src=\"media/higher_freq.gif\"  width = \"1200\" height = \"1600\"  />  \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
