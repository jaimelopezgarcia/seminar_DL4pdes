{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc48c41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contents\n",
    "\n",
    "\n",
    "- [Introduction](#the_destination)\n",
    "  * [Why deep learning and pdes](#sub-heading)\n",
    "  * [Applications](#sub-heading)\n",
    "  * [Solving PDEs with DL, overview](#sub-sub-heading)\n",
    "- [DL, a modern recipe](#heading-1)\n",
    "  * [DL basics](#sub-heading-1)\n",
    "  * [Backpropagation (it's just good'ol adjoint method)](#sub-heading-1)\n",
    "  * [Going deep, main problems, main solutions](#sub-heading-1)\n",
    "  * [Practicioner recipe](#sub-heading-1)\n",
    "- [Learning from data](#heading-2)\n",
    "  * [Problem Statement](#sub-heading-2)\n",
    "  * [Kernel Learning](#sub-sub-heading-2)\n",
    "  * [Case study Allen Cahn](#sub-sub-heading-2)\n",
    "  * [Multiple scales](#whatever)\n",
    "  * [Results](#hehe)\n",
    "      + [Validation dataset time predictions](#hhoo)\n",
    "      + [Phase plots](#hehe)\n",
    "      + [Time errors](#hoho)\n",
    "      + [Resolution dependency](#hoho)\n",
    "      + [Multiiple scales](#hehe)\n",
    "  * [Code disection](#huhu)\n",
    "- [Unsupervised Learning of the PDE, PINNS](#huhu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0727b38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "## Why Deep Learning and PDEs\n",
    "\n",
    "\n",
    "Deep learning fenomenal tool to learn arbitrarly complex functions byembedding data intro a continuous manifold that encode in much  lower dimensional space the main directions of variation of the data diistribution\n",
    "<img width=\"600\" height=\"600\" style = \"float : right\" src=\"media/Intro_manifold.png\">\n",
    "\n",
    "If this works with data with a much tangible stucture as music and images, it is expected to perform even better with mathematical objects that are much more likely to live in a continuous smooth manifold, with principal modes of evolution, as it is the case with the solution of PDEs equations \n",
    "<img width=\"600\" height=\"600\" style = \"float : left\" src=\"media/intro_modes.gif\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2fe9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "## Applications\n",
    "\n",
    "The biggest contributions of DL to the solution is the potential reduction in orders of magnitude the time complexity of finding a solution.This entails inmediate applications:\n",
    "   * Inverse problems\n",
    "   * Control processes\n",
    "   * Computer graphics\n",
    "   * Fast prototyping, assisted engineering.\n",
    "\n",
    "Other contributions beside this should be mentioned\n",
    "    - Explainability green ops, coherent structures etc\n",
    "    - Data assimilation, data library, bessel etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a2475",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "## How do we solve PDEs with deep learning\n",
    "### Two general approaches\n",
    "\n",
    "   - Supervised learning approach: Sample data from the population of solutions, and make the nerual network learn the mapping $NN: parameter \\rightarrow solution$\n",
    "   - Weighted residuals approach: Reparametrice $ N_{p}(y,\\frac{\\partial y}{\\partial x},...) = 0 $ with a neural network $\\hat{y}(x,\\theta)$ and minimize the residues of the associated functional along with the BCs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af004400",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning, a modern recipe\n",
    "## Neural network basics\n",
    "### Dont fix the basis, learn it\n",
    "\n",
    "Example, least squares regression problem $ \\underset{w}{argmin}L(w) = \\underset{w}{argmin}\\sum_{Data}|y-\\hat{y}(x,w)|^{2}$\n",
    " * Feature engenering, \"shallow learning\": $\\hat{y} = \\sum_{i}w_{i}\\phi_{i}(x) $\n",
    " * Neural networks : $\\hat{y} = \\sum_{i}w_{i}\\phi_{i}(x, w) $\n",
    " \n",
    "Instead of just addition as a building mechanism, NN use composition of blocks of non linear functions and linear applications called layers.\n",
    "\\begin{equation}\n",
    "\\phi(w,x) = \\sigma(W_{L} \\sigma(W_{L-1}....\\sigma(W_{1}X) )\n",
    "\\end{equation}\n",
    " \n",
    "To optimize we need to calculate the gradients $\\partial_{w}L$ respect to the weights of every layer. The algorithm that does it is a special implementation  of the adjoint method, called backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38a9b61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning, a modern recipe\n",
    "## Neural network basics\n",
    "### Calculating gradients\n",
    "#### Reformulation as a constrained optimization problem\n",
    "\n",
    "\\begin{array}{lc}\n",
    "\\mbox{J}: \\underset{p}{\\mbox{minimize}} & \\sum(x_{Data}-\\hat{x}(x_{input},p))^{2}  \\\\\n",
    "\\end{array}\n",
    "\\begin{array}{lm}\n",
    "\\mbox{G}: &   \\begin{cases} \n",
    "      \\hat{x} = \\sigma(Wx_{k})  \\\\\n",
    "      x_{k} = \\sigma(W_{k}x_{k-1}) \\\\\n",
    "      \\vdots \\\\\n",
    "      x_{2} = \\sigma(W_{2}x_{1})\\\\\n",
    "      x_{1} = \\sigma(W_{1}x_{input}) \\\\\n",
    "   \\end{cases} \\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801007cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning, a modern recipe\n",
    "## Neural network basics\n",
    "### Calculating gradients\n",
    "#### Adjoint method\n",
    "\\begin{array}{lc}\n",
    "\\underset{p}{\\mbox{minimize}} & J(x_{T},p)  \\\\\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{lcm}\n",
    "\\mbox{forward system} & g(x,p) = 0 = &   \\begin{cases} \n",
    "      g(x_{T},x_{T-1},p) = 0  \\\\\n",
    "      g(x_{T-1},x_{T-2},p) = 0 \\\\\n",
    "      \\vdots \\\\\n",
    "      g(x_{2},x_{1},p) = 0\\\\\n",
    "      g(x_{1},x_{0},p) = 0 \\\\\n",
    "      g(x_{0},p) = 0\n",
    "   \\end{cases} \\\\\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "J_{aug}(x_{T},p) = J(x_{T},p) + \\lambda_{T}g(x_{T},x_{T-1},p) + \\lambda_{T-1}g(x_{T-1},x_{T-2},p) \\ldots \\lambda_{1}g(x_{1},x_{0},p)+ \\lambda_{0}g(x_{0},p)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{array}{lm}\n",
    "\\mbox{backward/adjoint system} & \\begin{cases}\n",
    "        \\partial_{x}J+\\lambda_{T}\\partial_{x_{T}}g_{T} = 0 \\\\\n",
    "        \\lambda_{T}\\partial_{x_{T-1}}g_{T}+\\lambda_{T-1}\\partial_{x_{T-1}}g_{T-1} =0 \\\\\n",
    "        \\vdots \\\\\n",
    "        \\lambda_{1}\\partial_{x_{0}}g_{1}+\\lambda_{0}\\partial_{x_{0}}g_{0} = 0 \\\\\n",
    "        \\end{cases} \\\\\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{lm}\n",
    "\\mbox{Gradient} & \\begin{equation}\n",
    "        d_{p} J_{aug} = \\partial_{p} J +\n",
    "        \\lambda_{T}\\partial_{p}g_{T}+\\lambda_{T-1}\\partial_{p}g_{T-1}+\\ldots +\n",
    "        \\lambda_{1}\\partial_{p}g_{1}+\n",
    "        \\lambda_{0}\\partial_{p}g_{0}\n",
    "        \\end{equation}\n",
    "\\end{array}\n",
    "\n",
    "if we use $g_{L} =x_{L}-\\sigma(W_{L}x_{L-1}) = 0$ and clear $\\lambda$ we get the $Backpropagation$ algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a33e17",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning, a modern recipe\n",
    "## Neural network basics\n",
    "### Calculating gradients\n",
    "#### Adjoint method in time dependent problems, just for clarity\n",
    "\n",
    "\n",
    "\\begin{array}{lc}\n",
    "\\underset{p}{\\mbox{minimize}} & J(x(t),p) = \\int_{0}^{T} f(x(t),p) \\,dt  \\\\\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{lc}\n",
    "\\mbox{forward} & \\begin{cases} \n",
    "      g(\\dot{x},x,p) = 0  \\\\\n",
    "      g_{0}(x_{0},p) = 0\n",
    "   \\end{cases} \\\\\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{lc}\n",
    "\\mbox{backward/adjoint system} & \\begin{cases} \n",
    "       \\dot{\\lambda(\\tau)}\\partial_{\\dot{x}}g - \\lambda(\\tau)\\dot{\\partial_{\\dot{x}}}g + \\lambda(\\tau)\\partial_{x}g = -\\partial_{x}f  \\\\\n",
    "     \\lambda(\\tau = 0) = 0 \\\\\n",
    "      \\mu=\\lambda\\partial_{\\dot{x}}g\\partial_{x}g_{0}^{-1}\n",
    "   \\end{cases} \\\\\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{lc}\n",
    "\\mbox{Gradient} & \\begin{equation}\n",
    " d_{p}J_{aug} = \\int_{0}^{T}  \\partial_{p}f+\\lambda\\partial_{p}gdt+\\mu\\partial_{p}g_{0}\\Bigr|_{t = 0}\n",
    " \\end{equation} \\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da19395",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning, a modern recipe\n",
    "## Main problems, main solutions\n",
    "The main problem we encounter as we try to fit networks with increasing numbers of layers, is the explosion/vanishing variance of the activations and gradients of different layers.\n",
    " * PROBLEM: Decaying and exploding variance. Main techniques to stabilize the network and control variance:\n",
    "  * Proper initialization of weights.\n",
    "  * Normalization ( mostly batchnorm).\n",
    "  \n",
    "  \n",
    " * PROBLEM: Saturation.\n",
    "  * Residual connections\n",
    "  \n",
    "  \n",
    " * PROBLEM: Optimization in a bumpy loss landscape.\n",
    "  * Stochastic gradient descent with momentum , adaptative and annealed learning rate.\n",
    "  \n",
    "  \n",
    " * PROBLEM: Overfitting.\n",
    "  * Depending on the application it is not that much of a problem.\n",
    "  * Regularization, L2, dropout..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9400d67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning, a modern recipe\n",
    "## Recipe for building block/layer  and optimizer\n",
    "\n",
    "Use resnet (with different lengths) as a template and use as much capacity as you can afford.\n",
    "\n",
    "Main building block consist in layers with residual skip connections and batch normalization. If using Relu weights must be initializated with the proper scale ( He initialization). Scale input and labels and use Adam optimization with default parameters (pytorch), and small (1e-5) weight decay and reduce on plateau scheduler.\n",
    "\n",
    "<img width=\"300\" height=\"300\" style = \"float : left\" src=\"media/basic_block.jpg\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be4c9b47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Problem statement\n",
    "\n",
    "We consider the PDE $P$ with BC as a mapping $\\psi$ between function spaces where $X$ is the parameter space and $Y$ the solution space.\n",
    "$$P_{X}(y) = 0$$\n",
    "\n",
    "$F$ and $G$ are the operators that project the data to a discrete space. The symbol $\\varphi$ represent the mapping in the discrete space.\n",
    "\n",
    "\n",
    "![image.png](media/scheme_operator.jpg)\n",
    "\n",
    "If we work directly in the discretized space, we'll model the mapping with a convolutional neural network by minimizing:\n",
    "$$ \\underset{\\theta}{argmin} \\underset{x \\sim \\mu}{E}(Cost(\\varphi_{solver}(x)-\\hat{\\varphi}(x,\\theta))$$\n",
    "\n",
    "If we work in a function space we'll minimize:\n",
    "$$ \\underset{\\theta}{argmin} \\underset{x \\sim \\mu}{E}(Cost(\\psi_{solver}(x)-\\hat{\\psi}(x,\\theta))$$\n",
    "\n",
    "Both methods work with discrete data , but in the first case , we are learning directly a mapping  in $R^{N_{grid}}$ while in the second case we first project to a function space (fourier transform), we learn the mapping there and transform back to the discretized space.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0941871a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Case study : Evolutive system, Allen Cahn, spinodal decomposition\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "    \\partial_{t}u-M(\\Delta u -\\frac{1}{\\epsilon^{2}}(u^{2}-1)u) = 0 \\\\\n",
    "     u,\\nabla u |_{\\partial \\Omega} \\quad periodic \\\\\n",
    "     u(0,x,y) = u_{0}(x,y)\\\\\n",
    "     x,y\\in[0,1]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "  Gibs free energy vs phase            |  Initial condition, small fluctuations that trigger the decomposition\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"media/gibbs_potential.jpg\" alt=\"drawing\" width=\"400\" height=\"400\"  />  |  <img src=\"media/noise.png\" alt=\"drawing\" width=\"400\" height=\"400\"  />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716ad39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Case study : Evolutive system, Allen Cahn, spinodal decomposition\n",
    "\n",
    "  Simulations samples      |  $ M = 1,\\epsilon = 0.01, T = 200 dt$\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"media/sample1.gif\"   />  |  <img src=\"media/sample2.gif\"   />\n",
    "<img src=\"media/sample3.gif\"   />  |  <img src=\"media/sample4.gif\"   />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0979f07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Case study : Evolutive system, Allen Cahn, spinodal decomposition\n",
    "\n",
    "This is an interesting problem to learn beacuse without being chaotic , it exhibits multiple spatial and temporal timescales that must be solved simultaneously to accurattely predict long term behaviour.\n",
    "\n",
    "We have a extremely fast destabilization at the beggining that is followed by a slow evolution guided by the interface advances. Even if the coalescence stage is generally slow, when two drops are close to each other, the blending is fast, so it must be captured with a sufficiently small time step\n",
    "\n",
    "\n",
    "   time evolution   |  $E(abs(phase))\\quad vs \\quad time$\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"media/sample_decomp.gif\" width = \"400\" height = \"400\"  />  |  <img src=\"media/sample_decomp_phase.png\"  width = \"400\"  height = \"400\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "590b6152",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Models architecture\n",
    "\n",
    "   Image-Image CNN   |  Fourier Neural Operator\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"media/unetlike.png\" width = \"800\" height = \"300\"  />  |  <img src=\"media/fourier_operator.jpg\"  width = \"800\"  height = \"500\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e18fd8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Training\n",
    "\n",
    "\n",
    "The mapping we'll try to learn is $\\Psi: u_{T-\\Delta t}\\rightarrow u_{T} $ with the goal of applying it recurrently to predict evolution times much longer than $\\Delta$. As NN are not constrained by the time integration errors of traditional schemes, REFERENCE MULTISCALE, large $\\Delta t, 6*dt \\quad solver$.\n",
    "The objetive is to minize:\n",
    "$$\\underset{\\theta}{argmin}\\underset{u_{0},T}{E}(|u_{T+\\Delta t}-\\hat{u}_{T+\\Delta t}(\\theta, u_{T})|^{2})$$\n",
    "\n",
    "In both models, after the validation error crosed a treshold, more time steps ahead are predicted recurrently to increase stability, up to 4 timesteps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb3508",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning from data\n",
    "## Code dissection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d5c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
